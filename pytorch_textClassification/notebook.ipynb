{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b58914-fdf1-4423-b405-2d8b8f91bd1b",
   "metadata": {},
   "source": [
    "# PyTorch for Text Classification: Classifying IMDB Movie Reviews with TinyBERT Transformers\n",
    "This project explores the use of pytorch for text classification, specifically through classifying IMDB movie reviews with TinyBERT transformers. \n",
    "\n",
    "NOTE: This project is based on Codecademy's [text classification project](https://www.codecademy.com/content-items/29838c7636654e48ac72458af6373d5d).\n",
    "\n",
    "## Dataset\n",
    "The dataset is about movie reviews, and can be found at [Hugging Face](https://huggingface.co/datasets/Lowerated/lm6-movies-reviews-aspects); the given datasets in the `datasets` folder have been already cleaned and preprocessed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d90f6ef-0659-4afb-8b6e-dc40266b82b1",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275582c5-c2ca-43ae-85f0-5cdf88b36fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd5327e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42) # set random seed \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error() # remove warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5c280-304f-4038-8cb4-1c3c795d5fbd",
   "metadata": {},
   "source": [
    "## Import and Inspect the Movie Reviews\n",
    "The datasets are imported and analyzed preliminarily, to understand what datatypes are present, the meaning of each column, and the possible values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7219599-d548-4d36-9a76-73b861f9c5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>aspect</th>\n",
       "      <th>aspect_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ibiza filming location looks very enchanting</td>\n",
       "      <td>Cinematography</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RANDOLPH SCOTT always played men you could loo...</td>\n",
       "      <td>Characters</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>interesting and promising basic idea', 'some p...</td>\n",
       "      <td>Story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the film could explore very powerful politics,...</td>\n",
       "      <td>Story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The animation is nice, and the use of color ma...</td>\n",
       "      <td>Cinematography</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review          aspect  \\\n",
       "0       Ibiza filming location looks very enchanting  Cinematography   \n",
       "1  RANDOLPH SCOTT always played men you could loo...      Characters   \n",
       "2  interesting and promising basic idea', 'some p...           Story   \n",
       "3  the film could explore very powerful politics,...           Story   \n",
       "4  The animation is nice, and the use of color ma...  Cinematography   \n",
       "\n",
       "   aspect_encoded  \n",
       "0               0  \n",
       "1               1  \n",
       "2               2  \n",
       "3               2  \n",
       "4               0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_review_df = pd.read_csv(\"datasets/imdb_movie_reviews_train.csv\")\n",
    "\n",
    "train_review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a502b157-10a8-49a8-a83a-2020388fb013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 369 entries, 0 to 368\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   review          369 non-null    object\n",
      " 1   aspect          369 non-null    object\n",
      " 2   aspect_encoded  369 non-null    int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 8.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_review_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad97ce",
   "metadata": {},
   "source": [
    "The dataset has the following 3 columns:\n",
    "* \"review\": text content of the review\n",
    "* \"aspect\": themetic summary of what the review is about\n",
    "* \"aspect_encoded\": quantitative equivalent of the above column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a940209-b9f2-4390-be70-fa8c04d4b04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect\n",
      "Cinematography    125\n",
      "Characters        123\n",
      "Story             121\n",
      "Name: count, dtype: int64\n",
      "\n",
      " There are 3 possible labels.\n",
      "\n",
      "aspect_encoded\n",
      "0    125\n",
      "1    123\n",
      "2    121\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# look at the possible values of aspect column\n",
    "print(train_review_df[\"aspect\"].value_counts())\n",
    "\n",
    "n_aspects = train_review_df[\"aspect\"].nunique() # save the number of unique aspects\n",
    "print(f\"\\n There are {n_aspects} possible labels.\\n\")\n",
    "\n",
    "# confirm that the counts are equal to the aspect_encoded column\n",
    "print(train_review_df[\"aspect_encoded\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cfd0e3",
   "metadata": {},
   "source": [
    "Since there are 3 possible aspects, this is a multi-class classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17fa105-cdbd-406a-8cdd-961019222cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training body\n",
    "train_texts = train_review_df[\"review\"].tolist()\n",
    "train_labels = train_review_df[\"aspect_encoded\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273a602e-8f34-4276-9f8d-617a3046e957",
   "metadata": {},
   "source": [
    "## Pre-processing the Data\n",
    "The text data has to be pre-processed into a numerical representation to allow it to be fed to the model. This done through tokenization and truncation/padding. The preprocessed texts are then converted to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581436b8-1aea-444a-b8db-d4442b5bbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_review(review):\n",
    "    return re.findall(r'\\b\\w+\\b', review.lower())\n",
    "\n",
    "# tokenize all texts\n",
    "tokenized_train_texts = [tokenize_review(text) for text in train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0634d392-947d-4afe-bdf8-d32f0b6c3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of occurences of each token\n",
    "\n",
    "# list of all tokens\n",
    "combined_corpus = []\n",
    "for text in tokenized_train_texts:\n",
    "    for token in text:\n",
    "        combined_corpus.append(token)\n",
    "\n",
    "word_freqs = Counter(combined_corpus) # frequency of all tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18a4d0",
   "metadata": {},
   "source": [
    "Now, the vocabulary is created as the top 1000 most commonly occuring word tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7128789-c86d-4b6b-bce7-7973cc522009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common words are: \n",
      "[('the', 732), ('a', 307), ('and', 306), ('of', 296), ('is', 218), ('to', 213), ('in', 177), ('it', 134), ('s', 109), ('that', 105)]\n"
     ]
    }
   ],
   "source": [
    "# find the most common words\n",
    "MAX_VOCAB_SIZE = 1000\n",
    "most_common_words = word_freqs.most_common(MAX_VOCAB_SIZE)\n",
    "\n",
    "print(\"The 10 most common words are: \")\n",
    "print(most_common_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03088d1d-7c54-422e-a460-a75692fb4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab is created a dictionary with word: id of the most common words: 0 corresponds to unk (unknown word for tokens which are not common) and 1 corresponds to pad \n",
    "vocab = {word: id + 2 for id, (word, freq) in enumerate(most_common_words)}\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c5c4b",
   "metadata": {},
   "source": [
    "The texts can now be tokenized, encoded, and expressed as tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65d54afa-cf5f-4608-9a9d-c00acdcfa65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, vocab):\n",
    "    \"\"\"\n",
    "    Tokenizes and encodes each review text into a sequence of token IDs. \n",
    "\n",
    "    Args:\n",
    "    text (str) - review text to be tokenized and encoded\n",
    "    vocab (dict) - vocabulary as {token: code}\n",
    "\n",
    "    Returns:\n",
    "    encoded_text (list of ints)\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_text = tokenize_review(text)\n",
    "\n",
    "    encoded_text = []\n",
    "    for token in tokenized_text:\n",
    "        if token in vocab:\n",
    "            encoded_text.append(vocab[token])\n",
    "        else:\n",
    "            encoded_text.append(vocab['<unk>'])\n",
    "    \n",
    "    return encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3db3bfec-6d34-4220-82ca-202af4028256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(encoded_text, max_len):\n",
    "    \"\"\"\n",
    "    Pre-processes the encoded text to have the same length specified by the maximum length specified. \n",
    "\n",
    "    Args:\n",
    "    encoded_text (list of ints) - input encoded review text\n",
    "    max_len (int) - maximum length specified\n",
    "\n",
    "    Returns:\n",
    "    list of ints (subset or superset of encoded_text)\n",
    "    \"\"\"\n",
    "\n",
    "    if len(encoded_text) > max_len:\n",
    "        return encoded_text[:max_len]\n",
    "    else:\n",
    "        return encoded_text + [1]*(max_len - len(encoded_text)) # 1 corresponds to value for '<pad>' token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6c5c039-bdbb-494f-86a5-8947e7ee84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# take all texts, encode them and pad/truncate based on MAX_SEQ_LENGTH using the helper functions defined above\n",
    "padded_text_seqs = []\n",
    "for text in train_texts:\n",
    "    encoded_text = encode_text(text, vocab)\n",
    "    padded_text = pad_or_truncate(encoded_text, MAX_SEQ_LENGTH)\n",
    "    padded_text_seqs.append(padded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72fcb32d-e5da-4522-b143-93f4c31a0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text sequences to tensors\n",
    "X_tensor = torch.tensor(padded_text_seqs)\n",
    "y_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "# organize the input and label tensors to be loaded in batches\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10404cf-c27f-412f-aca9-c7f3a4ac813a",
   "metadata": {},
   "source": [
    "## Training a Simple Neural Network\n",
    "\n",
    "The first text classification model is a simple neural network with an embedding layer and a hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5557d49-7af6-4999-bdc2-e7d41b31d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) # set random seed --do not change!\n",
    "\n",
    "class SimpleNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        # initialize model with an embedding layer, a hidden layer, and the output\n",
    "        super(SimpleNNWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size)\n",
    "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the input\n",
    "        x = self.embedding(x)       # 1. input -> embedding layer\n",
    "        x = torch.mean(x, dim=1)    # 2. average the embedding into 1 representation\n",
    "        x = self.fc1(x)             # 3. averaged embedding output -> layer 1\n",
    "        x = torch.relu(x)           # 4. apply activation function\n",
    "        x = self.fc2(x)             # 5. relu output -> layer 2\n",
    "        return x                    # output as class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4df1e1a7-7232-4301-a5f8-6e16459e0490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNNWithEmbedding(\n",
      "  (embedding): Embedding(1002, 50)\n",
      "  (fc1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 50 \n",
    "hidden_size = 100 \n",
    "output_size = n_aspects\n",
    "\n",
    "text_classifier_nn = SimpleNNWithEmbedding(vocab_size, embed_size, hidden_size, output_size)\n",
    "print(text_classifier_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1926e79-6ac8-43a6-a0e5-e63315b09657",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(text_classifier_nn.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "63f3bea0-ab65-468d-b968-5d3fcf8f6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    # train the model for the given number of epochs\n",
    "    for i in range(num_epochs):\n",
    "        model.train() # set model to training mode\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # pass each batch of the loader through the model to train it \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()               # reset gradients at every pass\n",
    "            output = model(batch_X)             # forward pass\n",
    "            loss = criterion(output, batch_y)   # calculate loss\n",
    "            loss.backward()                     # backward pass\n",
    "            optimizer.step()                    # change weights and biases\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"[Epoch {i + 1}/{num_epochs}], Average CE Loss: {avg_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "75dae018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/50], Average CE Loss: 0.98320\n",
      "[Epoch 10/50], Average CE Loss: 0.74018\n",
      "[Epoch 15/50], Average CE Loss: 0.40746\n",
      "[Epoch 20/50], Average CE Loss: 0.15354\n",
      "[Epoch 25/50], Average CE Loss: 0.07105\n",
      "[Epoch 30/50], Average CE Loss: 0.03629\n",
      "[Epoch 35/50], Average CE Loss: 0.02796\n",
      "[Epoch 40/50], Average CE Loss: 0.01198\n",
      "[Epoch 45/50], Average CE Loss: 0.01633\n",
      "[Epoch 50/50], Average CE Loss: 0.01190\n"
     ]
    }
   ],
   "source": [
    "train_model(text_classifier_nn, train_dataloader, criterion, optimizer, num_epochs=50) # train model over 50 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f8b196-6de4-4d6d-9257-88adb9bb9a94",
   "metadata": {},
   "source": [
    "### Evaluate Neural Network\n",
    "The neural network will be evaluted based on a testing set (which is first imported and preprocessed as with the training set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f61ab47-9a7a-41f2-99f4-4e513a46e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test set\n",
    "test_review_df = pd.read_csv(\"datasets/imdb_movie_reviews_test.csv\")\n",
    "\n",
    "test_texts = test_review_df[\"review\"].tolist()\n",
    "test_labels = test_review_df[\"aspect_encoded\"].tolist()\n",
    "\n",
    "padded_text_seqs_test = []\n",
    "for text in test_texts:\n",
    "    encoded_text = encode_text(text, vocab)\n",
    "    padded_text = pad_or_truncate(encoded_text, MAX_SEQ_LENGTH)\n",
    "    padded_text_seqs_test.append(padded_text)\n",
    "\n",
    "X_test_tensor = torch.tensor(padded_text_seqs_test)\n",
    "y_test_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fbe3fcaa-7c53-40df-b801-78469db1e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_probabilities(model, test_dataloader):\n",
    "\n",
    "    model.eval() # set model to eval \n",
    "\n",
    "    all_probs = [] # stores all of the predicted probabilities for the testing dataset\n",
    "    all_labels = [] # stores all of the predicted labels for the testing dataset\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_dataloader:\n",
    "            outputs = model(batch_X)                        # get outputs from forward pass\n",
    "            probs = F.softmax(outputs, dim=1)               # generate predicted probabilities\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            predicted_labels = torch.argmax(outputs, dim=1) # select class label w highest probability\n",
    "            all_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    return all_probs, all_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c6cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs, pred_labels = get_predictions_and_probabilities(text_classifier_nn, test_dataloader) # get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce5406-f843-40c3-9078-3e4e32c71955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[27 18  4]\n",
      " [ 1 35  2]\n",
      " [ 2 15 28]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.55      0.68        49\n",
      "           1       0.51      0.92      0.66        38\n",
      "           2       0.82      0.62      0.71        45\n",
      "\n",
      "    accuracy                           0.68       132\n",
      "   macro avg       0.75      0.70      0.68       132\n",
      "weighted avg       0.76      0.68      0.69       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "conf_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "print(\"Confusion matrix:\\n\", conf_matrix)\n",
    "\n",
    "report = classification_report(test_labels, pred_labels)\n",
    "print(\"\\nClassification Report\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deb9780",
   "metadata": {},
   "source": [
    "Based on these values, the neural network had an overall accuracy of 68%, which is okay. There is room for improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934abae6-9de7-42b4-8c8b-5c0df23e84a6",
   "metadata": {},
   "source": [
    "## Fine-tuning a TinyBERT Transformer\n",
    "A TinyBERT model will be fine-tuned to classify movie reviews, in the hopes that it performs better than a simple neural network. This is loaded from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fda1b125-2c19-4b9a-b5fb-84166724daec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Michelle\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Michelle\\.cache\\huggingface\\hub\\models--huawei-noah--TinyBERT_General_4L_312D. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained tinyBERT model \n",
    "model_name = 'huawei-noah/TinyBERT_General_4L_312D'\n",
    "\n",
    "tinybert_tokenizer = BertTokenizer.from_pretrained(model_name) # tokenizer\n",
    "text_classifier_tinybert = BertForSequenceClassification.from_pretrained(model_name, num_labels=n_aspects) # model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1882aba4-632b-4043-bd85-47e13e4e47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all parameters in pre-trained tinyBERT\n",
    "for param in text_classifier_tinybert.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze the classification layer added on top of the pre-trained model\n",
    "for param in text_classifier_tinybert.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# unfreeze the encoder layer specified at layer[3]\n",
    "for param in text_classifier_tinybert.bert.encoder.layer[3].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b7004e87-7a64-4507-8da5-6f914388a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH_TINYBERT = 124\n",
    "# create tokenized training set\n",
    "X_train = tinybert_tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=MAX_SEQ_LENGTH_TINYBERT)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train[\"input_ids\"], X_train[\"attention_mask\"], y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "952d9821-1780-4d4c-82d6-1d3edb48fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, text_classifier_tinybert.parameters()), lr=0.0025)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0737017b-f254-4da6-a291-cd0ac2685be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.000056820611159\n",
      "Epoch 2/10, Loss: 0.5841939138869444\n",
      "Epoch 3/10, Loss: 0.39991521059225005\n",
      "Epoch 4/10, Loss: 0.35998631330827874\n",
      "Epoch 5/10, Loss: 0.33731860884775716\n",
      "Epoch 6/10, Loss: 0.23576063445458809\n",
      "Epoch 7/10, Loss: 0.3799276165664196\n",
      "Epoch 8/10, Loss: 0.30424073167766136\n",
      "Epoch 9/10, Loss: 0.27067273389548063\n",
      "Epoch 10/10, Loss: 0.21834331766391793\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for i in range(num_epochs):\n",
    "    text_classifier_tinybert.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch_X, batch_attention_mask, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = text_classifier_tinybert(input_ids=batch_X, attention_mask=batch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, batch_y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {i+1}/{num_epochs}, Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e6365",
   "metadata": {},
   "source": [
    "### Evaluate TinyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2a7403a5-7c72-47dc-9444-2314cecf16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare test set\n",
    "X_test = tinybert_tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=MAX_SEQ_LENGTH_TINYBERT)\n",
    "y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "test_dataset = TensorDataset(X_test['input_ids'], X_test['attention_mask'], y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "336a679d-ab59-4632-a7db-5f746955e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions for test set\n",
    "text_classifier_tinybert.eval()\n",
    "pred_probs = []\n",
    "pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_attention_mask, batch_y in test_loader:\n",
    "        outputs = text_classifier_tinybert(input_ids= batch_X, attention_mask= batch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_probs.extend(probs.cpu().numpy())\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        pred_labels.extend(predicted_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cb920469-ad9c-4010-ade7-f46a201aaf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      " [[45  0  4]\n",
      " [ 1 35  2]\n",
      " [ 0  2 43]]\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95        49\n",
      "           1       0.95      0.92      0.93        38\n",
      "           2       0.88      0.96      0.91        45\n",
      "\n",
      "    accuracy                           0.93       132\n",
      "   macro avg       0.93      0.93      0.93       132\n",
      "weighted avg       0.93      0.93      0.93       132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "conf_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "print(\"Confusion matrix:\\n\", conf_matrix)\n",
    "\n",
    "report = classification_report(test_labels, pred_labels)\n",
    "print(\"\\nClassification Report\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e943db",
   "metadata": {},
   "source": [
    "The TinyBERT model has an accuracy of 93%. This is quite good, and almost 50% better than the simple neural network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
